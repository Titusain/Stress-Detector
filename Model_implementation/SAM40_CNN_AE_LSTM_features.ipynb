{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMTOCsB6Zhz_",
        "outputId": "5aa93000-e8ec-4e75-932b-818f2d1133d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling1D, Flatten, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, LSTM, RepeatVector, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "import os\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ohV4zQrsZy3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CNNImplementation(df):\n",
        "\n",
        "    # Set window size\n",
        "    window_size = 10\n",
        "    num_features = 20\n",
        "\n",
        "    X = df.drop(columns=['stress']).values\n",
        "    y = df['stress'].values\n",
        "\n",
        "    # Iterate over data with step 1 and extract windows\n",
        "    X = [X[i:i+window_size] for i in range(0, len(X) - window_size + 1, window_size)]\n",
        "    y = [y[i:i+window_size] for i in range(0, len(y) - window_size + 1, window_size)]\n",
        "\n",
        "    # Convert windows to numpy array\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "\n",
        "    out_list = []\n",
        "\n",
        "    # Iterate over input array\n",
        "    for subarr in y:\n",
        "        if np.all(subarr == 1):\n",
        "            out_list.append([1])\n",
        "        elif np.all(subarr == 0):\n",
        "            out_list.append([0])\n",
        "\n",
        "\n",
        "    out_arr = []\n",
        "    for x in out_list:\n",
        "        out_arr.extend([x for i in range(int(X.shape[2]/num_features))])\n",
        "\n",
        "    print(\"X.shape: {} and y.shape: {}\".format(X.shape, y.shape))\n",
        "\n",
        "    # Convert output list to numpy array\n",
        "    out_arr = np.array(out_arr)\n",
        "    y = out_arr\n",
        "    X = X.reshape(-1, window_size, num_features, 1)\n",
        "\n",
        "    print(\"X.shape: {} and y.shape: {}\".format(X.shape, y.shape))\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
        "\n",
        "\n",
        "    # Define the CNN model\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(window_size,num_features,1)),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='softmax')\n",
        "\n",
        "    ])\n",
        "    # Compile the model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data = (X_test, y_test))\n",
        "\n",
        "    loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "jqTFCvEtaJuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def AE_LSTM_implementation(df):\n",
        "    X = df.drop(columns=['stress']).values\n",
        "    y = df['stress'].values\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state = 42)\n",
        "\n",
        "    X_train = tf.convert_to_tensor(X_train)\n",
        "    X_train = tf.expand_dims(X_train, axis=1)\n",
        "    X_test = tf.convert_to_tensor(X_test)\n",
        "    X_test = tf.expand_dims(X_test, axis=1)\n",
        "\n",
        "    # Define the input shape\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "    # Define the size of the bottleneck layer\n",
        "    latent_dim = 64\n",
        "\n",
        "    # Define the encoder\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = LSTM(latent_dim, activation='relu')(inputs)\n",
        "    n = K.int_shape(x)[1]\n",
        "    encoded = RepeatVector(n)(x)\n",
        "\n",
        "    # Define the decoder\n",
        "    decoded = LSTM(input_shape[1], activation='relu', return_sequences=True)(encoded)\n",
        "\n",
        "    # Combine the encoder and decoder into an autoencoder model\n",
        "    autoencoder = Model(inputs, decoded)\n",
        "\n",
        "    # Compile the autoencoder model\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Train the autoencoder model\n",
        "    autoencoder.fit(X_train, X_train, epochs=1, batch_size=32, validation_split=0.1)\n",
        "\n",
        "    # Extract the bottleneck layer\n",
        "    encoder = Model(inputs, encoded)\n",
        "    encoded_train_data = encoder.predict(X_train)\n",
        "\n",
        "    # Define the LSTM classifier\n",
        "    lstm_input = Input(shape=(encoded_train_data.shape[1], encoded_train_data.shape[2]))\n",
        "\n",
        "    x = LSTM(64, return_sequences=True)(lstm_input)\n",
        "    outputs = LSTM(1, activation='sigmoid', return_sequences=True)(x)\n",
        "\n",
        "    classifier = Model(lstm_input, outputs)\n",
        "\n",
        "    # Compile the classifier model\n",
        "    classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the classifier model using the encoded training data\n",
        "    classifier.fit(encoded_train_data, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OnqhBSbnrpB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mergeCSV(src, path):\n",
        "\n",
        "  # Merging the files\n",
        "  joined_files = os.path.join(src, path)\n",
        "\n",
        "  # A list of all joined files is returned\n",
        "  joined_list = glob(joined_files)\n",
        "\n",
        "  # Finally, the files are joined\n",
        "  df = pd.concat(map(pd.read_csv, joined_list), ignore_index=True)\n",
        "\n",
        "  # Save in specified location\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "dLk0mzpWZ6Sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_corr_filter(df, threshold):\n",
        "\n",
        "    # Calculate correlation matrix\n",
        "    corr_matrix = df.corr().abs()\n",
        "\n",
        "    # Select upper triangle of correlation matrix\n",
        "    upper = corr_matrix.where(pd.np.triu(pd.np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "    # Find features with correlation above threshold\n",
        "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
        "\n",
        "    # Drop highly correlated features\n",
        "    filtered_df = df.drop(to_drop, axis=1)\n",
        "\n",
        "    return filtered_df\n"
      ],
      "metadata": {
        "id": "aovMyynFzhmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "location = r\"/content/gdrive/My Drive/SAM40/Preprocessing_own/Preprocessed_filtered_data_features/Time_Frequency_domain\"\n",
        "\n",
        "stressdata = mergeCSV(\n",
        "    location + r\"/Arithmetic*\",\n",
        "    r\"Arithmetic*.csv\"\n",
        ")\n",
        "if 'Unnamed: 0.1' in stressdata.columns:\n",
        "    stressdata.drop(['Unnamed: 0.1'], axis=1, inplace=True)\n",
        "if 'Unnamed: 0' in stressdata.columns:\n",
        "    stressdata.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
        "\n",
        "stressdata['stress'] = 1\n",
        "\n",
        "relaxdata = mergeCSV(\n",
        "    location + r\"/Relax*\",\n",
        "    r\"Relax*.csv\"\n",
        ")\n",
        "\n",
        "if 'Unnamed: 0.1' in relaxdata.columns:\n",
        "    relaxdata.drop(['Unnamed: 0.1'], axis=1, inplace=True)\n",
        "if 'Unnamed: 0' in relaxdata.columns:\n",
        "    relaxdata.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
        "\n",
        "relaxdata['stress'] = 0\n",
        "\n",
        "data = pd.concat([stressdata, relaxdata])\n"
      ],
      "metadata": {
        "id": "73eiE6SeZ8fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = apply_corr_filter(data, 0.7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Q60dwWTw17O",
        "outputId": "f2e335cc-299f-4752-c48a-fcb48d6ffa3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-0ef0483afea9>:7: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
            "  upper = corr_matrix.where(pd.np.triu(pd.np.ones(corr_matrix.shape), k=1).astype(bool))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UlT8nZqzwU8",
        "outputId": "63c1d5a9-e415-4ed1-9b04-52d55986ac9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6000, 21)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(location, \": \")\n",
        "\n",
        "print(CNNImplementation(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU5bh80UclXP",
        "outputId": "64d6aa4b-af1b-4863-b0a2-2c6634359183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/SAM40/Preprocessing_own/Preprocessed_filtered_data_features/Time_Frequency_domain : \n",
            "X.shape: (600, 10, 20) and y.shape: (600, 10)\n",
            "X.shape: (600, 10, 20, 1) and y.shape: (600, 1)\n",
            "Epoch 1/10\n",
            "15/15 [==============================] - 2s 35ms/step - loss: 1.0733 - accuracy: 0.4958 - val_loss: 0.8593 - val_accuracy: 0.5167\n",
            "Epoch 2/10\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.6973 - accuracy: 0.4958 - val_loss: 0.7168 - val_accuracy: 0.5167\n",
            "Epoch 3/10\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.7141 - accuracy: 0.4958 - val_loss: 0.7131 - val_accuracy: 0.5167\n",
            "Epoch 4/10\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6643 - accuracy: 0.4958 - val_loss: 0.7574 - val_accuracy: 0.5167\n",
            "Epoch 5/10\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6380 - accuracy: 0.4958 - val_loss: 0.7406 - val_accuracy: 0.5167\n",
            "Epoch 6/10\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6598 - accuracy: 0.4958 - val_loss: 0.8533 - val_accuracy: 0.5167\n",
            "Epoch 7/10\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6594 - accuracy: 0.4958 - val_loss: 0.7603 - val_accuracy: 0.5167\n",
            "Epoch 8/10\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6678 - accuracy: 0.4958 - val_loss: 0.7422 - val_accuracy: 0.5167\n",
            "Epoch 9/10\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.6061 - accuracy: 0.4958 - val_loss: 0.7399 - val_accuracy: 0.5167\n",
            "Epoch 10/10\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.6042 - accuracy: 0.4958 - val_loss: 0.7385 - val_accuracy: 0.5167\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.7385 - accuracy: 0.5167\n",
            "0.5166666507720947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AE_LSTM_implementation(data)"
      ],
      "metadata": {
        "id": "81Csqq_gjswN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4604cd3-de24-45e8-863a-9ed9e537da8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "127/127 [==============================] - 11s 60ms/step - loss: 287.1279 - val_loss: 208.5883\n",
            "141/141 [==============================] - 0s 2ms/step\n",
            "Epoch 1/10\n",
            "127/127 [==============================] - 14s 81ms/step - loss: 0.6982 - accuracy: 0.5069 - val_loss: 0.6944 - val_accuracy: 0.4935\n",
            "Epoch 2/10\n",
            "127/127 [==============================] - 8s 61ms/step - loss: 0.6940 - accuracy: 0.4996 - val_loss: 0.6931 - val_accuracy: 0.5156\n",
            "Epoch 3/10\n",
            "127/127 [==============================] - 9s 72ms/step - loss: 0.6937 - accuracy: 0.4975 - val_loss: 0.6932 - val_accuracy: 0.5070\n",
            "Epoch 4/10\n",
            "127/127 [==============================] - 9s 73ms/step - loss: 0.6934 - accuracy: 0.4932 - val_loss: 0.6931 - val_accuracy: 0.5126\n",
            "Epoch 5/10\n",
            "127/127 [==============================] - 8s 60ms/step - loss: 0.6933 - accuracy: 0.5006 - val_loss: 0.6932 - val_accuracy: 0.5099\n",
            "Epoch 6/10\n",
            "127/127 [==============================] - 10s 75ms/step - loss: 0.6933 - accuracy: 0.4995 - val_loss: 0.6932 - val_accuracy: 0.5108\n",
            "Epoch 7/10\n",
            "127/127 [==============================] - 9s 70ms/step - loss: 0.6933 - accuracy: 0.4991 - val_loss: 0.6931 - val_accuracy: 0.5113\n",
            "Epoch 8/10\n",
            "127/127 [==============================] - 9s 68ms/step - loss: 0.6932 - accuracy: 0.4970 - val_loss: 0.6931 - val_accuracy: 0.5129\n",
            "Epoch 9/10\n",
            "127/127 [==============================] - 10s 75ms/step - loss: 0.6932 - accuracy: 0.5030 - val_loss: 0.6931 - val_accuracy: 0.5131\n",
            "Epoch 10/10\n",
            "127/127 [==============================] - 8s 61ms/step - loss: 0.6932 - accuracy: 0.5030 - val_loss: 0.6931 - val_accuracy: 0.5132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    #Implementation of state of art deep learning algorithm"
      ],
      "metadata": {
        "id": "U122gkz6sMVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "Y_xUjJ-8UQTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def AE_LSTM_impli(df):\n",
        "    X = df.drop(columns=['stress']).values\n",
        "    y = df['stress'].values\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "    # Normalize the input data\n",
        "    scaler = MinMaxScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    X_train = tf.expand_dims(X_train, axis=2)\n",
        "    X_test = tf.expand_dims(X_test, axis=2)\n",
        "\n",
        "    # Define the input shape\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "    # Define the size of the bottleneck layer\n",
        "    latent_dim = 64\n",
        "\n",
        "    # Define the encoder\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = LSTM(latent_dim, activation='relu')(inputs)\n",
        "    encoded = RepeatVector(input_shape[0])(x)\n",
        "\n",
        "    # Define the decoder\n",
        "    decoded = LSTM(input_shape[1], activation='relu', return_sequences=True)(encoded)\n",
        "\n",
        "    # Combine the encoder and decoder into an autoencoder model\n",
        "    autoencoder = Model(inputs, decoded)\n",
        "\n",
        "    # Compile the autoencoder model\n",
        "    autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Train the autoencoder model\n",
        "    autoencoder.fit(X_train, X_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "    # Extract the bottleneck layer\n",
        "    encoder = Model(inputs, encoded)\n",
        "    encoded_train_data = encoder.predict(X_train)\n",
        "\n",
        "    # Define the LSTM classifier\n",
        "    lstm_input = Input(shape=(encoded_train_data.shape[1], encoded_train_data.shape[2]))\n",
        "\n",
        "    x = LSTM(64, return_sequences=True)(lstm_input)\n",
        "    outputs = LSTM(1, activation='sigmoid')(x)\n",
        "\n",
        "    classifier = Model(lstm_input, outputs)\n",
        "\n",
        "    # Compile the classifier model\n",
        "    classifier.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
        "\n",
        "    # Train the classifier model using the encoded training data\n",
        "    classifier.fit(encoded_train_data, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "    # Evaluate the classifier model on the test data\n",
        "    encoded_test_data = encoder.predict(X_test)\n",
        "    loss, accuracy = classifier.evaluate(encoded_test_data, y_test)\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "RnG0zIiUOkH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AE_LSTM_impli(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmxNGRrUURpR",
        "outputId": "b3c42309-3c23-454f-a697-26cf13712989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "127/127 [==============================] - 9s 38ms/step - loss: 0.0676 - val_loss: 0.0510\n",
            "Epoch 2/10\n",
            "127/127 [==============================] - 2s 20ms/step - loss: 0.0502 - val_loss: 0.0503\n",
            "Epoch 3/10\n",
            "127/127 [==============================] - 3s 20ms/step - loss: 0.0499 - val_loss: 0.0499\n",
            "Epoch 4/10\n",
            "127/127 [==============================] - 4s 30ms/step - loss: 0.0498 - val_loss: 0.0498\n",
            "Epoch 5/10\n",
            "127/127 [==============================] - 3s 23ms/step - loss: 0.0495 - val_loss: 0.0496\n",
            "Epoch 6/10\n",
            "127/127 [==============================] - 3s 21ms/step - loss: 0.0493 - val_loss: 0.0497\n",
            "Epoch 7/10\n",
            "127/127 [==============================] - 3s 20ms/step - loss: 0.0492 - val_loss: 0.0493\n",
            "Epoch 8/10\n",
            "127/127 [==============================] - 3s 21ms/step - loss: 0.0490 - val_loss: 0.0491\n",
            "Epoch 9/10\n",
            "127/127 [==============================] - 4s 31ms/step - loss: 0.0489 - val_loss: 0.0491\n",
            "Epoch 10/10\n",
            "127/127 [==============================] - 3s 22ms/step - loss: 0.0488 - val_loss: 0.0490\n",
            "141/141 [==============================] - 1s 5ms/step\n",
            "Epoch 1/10\n",
            "127/127 [==============================] - 7s 27ms/step - loss: 0.2511 - accuracy: 0.4928 - val_loss: 0.2497 - val_accuracy: 0.5156\n",
            "Epoch 2/10\n",
            "127/127 [==============================] - 3s 20ms/step - loss: 0.2505 - accuracy: 0.5037 - val_loss: 0.2504 - val_accuracy: 0.4844\n",
            "Epoch 3/10\n",
            "127/127 [==============================] - 4s 35ms/step - loss: 0.2508 - accuracy: 0.4909 - val_loss: 0.2501 - val_accuracy: 0.4844\n",
            "Epoch 4/10\n",
            "127/127 [==============================] - 3s 22ms/step - loss: 0.2503 - accuracy: 0.4985 - val_loss: 0.2498 - val_accuracy: 0.5156\n",
            "Epoch 5/10\n",
            "127/127 [==============================] - 3s 22ms/step - loss: 0.2502 - accuracy: 0.5007 - val_loss: 0.2501 - val_accuracy: 0.4844\n",
            "Epoch 6/10\n",
            "127/127 [==============================] - 3s 22ms/step - loss: 0.2502 - accuracy: 0.4998 - val_loss: 0.2498 - val_accuracy: 0.5156\n",
            "Epoch 7/10\n",
            "127/127 [==============================] - 3s 27ms/step - loss: 0.2501 - accuracy: 0.5037 - val_loss: 0.2501 - val_accuracy: 0.4844\n",
            "Epoch 8/10\n",
            "127/127 [==============================] - 4s 31ms/step - loss: 0.2500 - accuracy: 0.4975 - val_loss: 0.2500 - val_accuracy: 0.4778\n",
            "Epoch 9/10\n",
            "127/127 [==============================] - 3s 21ms/step - loss: 0.2501 - accuracy: 0.4889 - val_loss: 0.2500 - val_accuracy: 0.5222\n",
            "Epoch 10/10\n",
            "127/127 [==============================] - 3s 22ms/step - loss: 0.2501 - accuracy: 0.4995 - val_loss: 0.2499 - val_accuracy: 0.5156\n",
            "47/47 [==============================] - 0s 5ms/step\n",
            "47/47 [==============================] - 0s 7ms/step - loss: 0.2501 - accuracy: 0.4873\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4873333275318146"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def CNNImpli(df):\n",
        "\n",
        "    # Set window size\n",
        "    window_size = 10\n",
        "    num_features = 20\n",
        "\n",
        "    X = df.drop(columns=['stress']).values\n",
        "    y = df['stress'].values\n",
        "\n",
        "    # Iterate over data with step 1 and extract windows\n",
        "    X = [X[i:i+window_size] for i in range(0, len(X) - window_size + 1, window_size)]\n",
        "    y = [y[i:i+window_size] for i in range(0, len(y) - window_size + 1, window_size)]\n",
        "\n",
        "    # Convert windows to numpy array\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "\n",
        "    out_list = []\n",
        "\n",
        "    # Iterate over input array\n",
        "    for subarr in y:\n",
        "        if np.all(subarr == 1):\n",
        "            out_list.append([1])\n",
        "        elif np.all(subarr == 0):\n",
        "            out_list.append([0])\n",
        "\n",
        "\n",
        "    out_arr = []\n",
        "    for x in out_list:\n",
        "        out_arr.extend([x for i in range(int(X.shape[2]/num_features))])\n",
        "\n",
        "    print(\"X.shape: {} and y.shape: {}\".format(X.shape, y.shape))\n",
        "\n",
        "    # Convert output list to numpy array\n",
        "    out_arr = np.array(out_arr)\n",
        "    y = out_arr\n",
        "    X = X.reshape(-1, window_size, num_features, 1)\n",
        "\n",
        "    print(\"X.shape: {} and y.shape: {}\".format(X.shape, y.shape))\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
        "\n",
        "\n",
        "    # Define the CNN model\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(window_size,num_features,1)),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "    ])\n",
        "    # Compile the model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data = (X_test, y_test))\n",
        "\n",
        "    loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "qSyFqllaUdNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CNNImpli(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lB4Ss6WXXhD3",
        "outputId": "75127cc0-9a64-4e46-f201-4b91230bfbca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X.shape: (600, 10, 20) and y.shape: (600, 10)\n",
            "X.shape: (600, 10, 20, 1) and y.shape: (600, 1)\n",
            "Epoch 1/10\n",
            "15/15 [==============================] - 2s 92ms/step - loss: 1.3138 - accuracy: 0.5333 - val_loss: 0.7026 - val_accuracy: 0.5500\n",
            "Epoch 2/10\n",
            "15/15 [==============================] - 1s 50ms/step - loss: 0.7256 - accuracy: 0.5708 - val_loss: 0.7424 - val_accuracy: 0.5083\n",
            "Epoch 3/10\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 0.6970 - accuracy: 0.5604 - val_loss: 0.8587 - val_accuracy: 0.5250\n",
            "Epoch 4/10\n",
            "15/15 [==============================] - 1s 47ms/step - loss: 0.6754 - accuracy: 0.5750 - val_loss: 0.7278 - val_accuracy: 0.5750\n",
            "Epoch 5/10\n",
            "15/15 [==============================] - 1s 48ms/step - loss: 0.6429 - accuracy: 0.6000 - val_loss: 0.7015 - val_accuracy: 0.5500\n",
            "Epoch 6/10\n",
            "15/15 [==============================] - 1s 50ms/step - loss: 0.6332 - accuracy: 0.6208 - val_loss: 0.7640 - val_accuracy: 0.5833\n",
            "Epoch 7/10\n",
            "15/15 [==============================] - 1s 39ms/step - loss: 0.6392 - accuracy: 0.6292 - val_loss: 0.7183 - val_accuracy: 0.5917\n",
            "Epoch 8/10\n",
            "15/15 [==============================] - 0s 33ms/step - loss: 0.6002 - accuracy: 0.7083 - val_loss: 0.7244 - val_accuracy: 0.5833\n",
            "Epoch 9/10\n",
            "15/15 [==============================] - 1s 43ms/step - loss: 0.5797 - accuracy: 0.6917 - val_loss: 0.7078 - val_accuracy: 0.5583\n",
            "Epoch 10/10\n",
            "15/15 [==============================] - 1s 40ms/step - loss: 0.5988 - accuracy: 0.6771 - val_loss: 0.7066 - val_accuracy: 0.5667\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.7066 - accuracy: 0.5667\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5666666626930237"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOO33TLk0VmI",
        "outputId": "8e2706ee-610c-43c9-8b5e-089d9ba707b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.20.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers.experimental import GraphConv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "eBg6HXIKzrVJ",
        "outputId": "bc4c350a-ecba-4e9c-8b29-fd2413cfae0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-e91aa1f85695>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraphConv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'GraphConv' from 'tensorflow.keras.layers.experimental' (/usr/local/lib/python3.10/dist-packages/keras/api/_v2/keras/layers/experimental/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def AE_CNN_GNN_implementation(df):\n",
        "  # Set window size\n",
        "            window_size = 10\n",
        "            num_features = 20\n",
        "\n",
        "            X = df.drop(columns=['stress']).values\n",
        "            y = df['stress'].values\n",
        "\n",
        "            # Iterate over data with step 1 and extract windows\n",
        "            X = [X[i:i+window_size] for i in range(0, len(X) - window_size + 1, window_size)]\n",
        "            y = [y[i:i+window_size] for i in range(0, len(y) - window_size + 1, window_size)]\n",
        "\n",
        "            # Convert windows to numpy array\n",
        "            X = np.array(X)\n",
        "            y = np.array(y)\n",
        "\n",
        "\n",
        "            out_list = []\n",
        "\n",
        "            # Iterate over input array\n",
        "            for subarr in y:\n",
        "                if np.all(subarr == 1):\n",
        "                    out_list.append([1])\n",
        "                elif np.all(subarr == 0):\n",
        "                    out_list.append([0])\n",
        "\n",
        "\n",
        "            out_arr = []\n",
        "            for x in out_list:\n",
        "                out_arr.extend([x for i in range(int(X.shape[2]/num_features))])\n",
        "\n",
        "            print(\"X.shape: {} and y.shape: {}\".format(X.shape, y.shape))\n",
        "\n",
        "            # Convert output list to numpy array\n",
        "            out_arr = np.array(out_arr)\n",
        "            y = out_arr\n",
        "            X = X.reshape(-1, window_size, num_features, 1)\n",
        "\n",
        "            print(\"X.shape: {} and y.shape: {}\".format(X.shape, y.shape))\n",
        "\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
        "\n",
        "\n",
        "            # Define the CNN model\n",
        "            model = tf.keras.Sequential([\n",
        "                tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(window_size,num_features,1)),\n",
        "                tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "                tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "                tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "                tf.keras.layers.Flatten(),\n",
        "                tf.keras.layers.Dense(64, activation='relu'),\n",
        "                tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "            ])\n",
        "            # Add a graph neural network layer\n",
        "            model.add(tf.keras.layers.GraphConv(64, (3, 3)))\n",
        "\n",
        "            # Compile the model\n",
        "            model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "            # Train the model\n",
        "            model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data = (X_test, y_test))\n",
        "\n",
        "            loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "            return accuracy\n"
      ],
      "metadata": {
        "id": "uoeRSE_7yOp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AE_CNN_GNN_implementation(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "bj-fZN-zw6vK",
        "outputId": "0ed1c9c8-0cc5-43de-db2c-e18ba7f93731"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X.shape: (600, 10, 20) and y.shape: (600, 10)\n",
            "X.shape: (600, 10, 20, 1) and y.shape: (600, 1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-c8741247b851>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mAE_CNN_GNN_implementation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-30ba5e8bb4f5>\u001b[0m in \u001b[0;36mAE_CNN_GNN_implementation\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     54\u001b[0m             ])\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# Add a graph neural network layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m# Compile the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.keras.layers' has no attribute 'GraphConv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def CNN_RNN_Impli(df):\n",
        "\n",
        "    # Set window size\n",
        "    window_size = 10\n",
        "    num_features = 20\n",
        "\n",
        "    X = df.drop(columns=['stress']).values\n",
        "    y = df['stress'].values\n",
        "\n",
        "    # Iterate over data with step 1 and extract windows\n",
        "    X = [X[i:i+window_size] for i in range(0, len(X) - window_size + 1, window_size)]\n",
        "    y = [y[i:i+window_size] for i in range(0, len(y) - window_size + 1, window_size)]\n",
        "\n",
        "    # Convert windows to numpy array\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    X = X[:, np.newaxis]\n",
        "    # out_list = []\n",
        "\n",
        "    # # Iterate over input array\n",
        "    # for subarr in y:\n",
        "    #     if np.all(subarr == 1):\n",
        "    #         out_list.append([1])\n",
        "    #     elif np.all(subarr == 0):\n",
        "    #         out_list.append([0])\n",
        "\n",
        "\n",
        "    # out_arr = []\n",
        "    # for x in out_list:\n",
        "    #     out_arr.extend([x for i in range(int(X.shape[2]/num_features))])\n",
        "\n",
        "    # print(\"X.shape: {} and y.shape: {}\".format(X.shape, y.shape))\n",
        "\n",
        "    # # Convert output list to numpy array\n",
        "    # out_arr = np.array(out_arr)\n",
        "    # y = out_arr\n",
        "    # X = X.reshape(-1, window_size, num_features, 1)\n",
        "\n",
        "    # print(\"X.shape: {} and y.shape: {}\".format(X.shape, y.shape))\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
        "\n",
        "\n",
        "    # Define the CNN model\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(window_size,num_features,1)),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Define the LSTM model\n",
        "    lstm_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.LSTM(64),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Combine the CNN and LSTM models\n",
        "    model = tf.keras.Sequential([\n",
        "        model,\n",
        "        lstm_model\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data = (X_test, y_test))\n",
        "\n",
        "    loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "m-DackYiw9ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_RNN_Impli(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "_kI-ZJPC2ACq",
        "outputId": "1f092f29-499b-418d-d668-60b535443aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-a2ab6841f5e2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCNN_RNN_Impli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-f470a099c03a>\u001b[0m in \u001b[0;36mCNN_RNN_Impli\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Combine the CNN and LSTM models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     model = tf.keras.Sequential([\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mlstm_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/trackable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    236\u001b[0m                     \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                     \u001b[0;34m\"is incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"sequential_14\" (type Sequential).\n\nInput 0 of layer \"lstm_6\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 1)\n\nCall arguments received by layer \"sequential_14\" (type Sequential):\n  • inputs=tf.Tensor(shape=(None, 1), dtype=float32)\n  • training=None\n  • mask=None"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TiceTbRx2Dk-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}